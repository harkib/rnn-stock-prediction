{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit0429f146496f4ea290801458aa87e618",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import pickle "
   ]
  },
  {
   "source": [
    "# Define Model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lstm_Classifier(nn.Module):\n",
    "    def __init__ (self, input_size, hidden_size, num_layers, batch_size, dropout, num_classes):\n",
    "        super(lstm_Classifier, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout= dropout)\n",
    "        self.fc = nn.Linear(hidden_size,num_classes)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        hidden = (torch.randn(self.num_layers, self.batch_size, self.hidden_size),\n",
    "                torch.randn(self.num_layers, self.batch_size, self.hidden_size))\n",
    "        return hidden\n",
    "\n",
    "    def forward(self, X, hidden = None): \n",
    "        \n",
    "        if hidden == None: \n",
    "            hidden = self.init_hidden()\n",
    "\n",
    "        X,_ = self.lstm(X, hidden)\n",
    "        X = self.fc(X[-1])\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 4\n",
    "num_layers = 2\n",
    "hidden_size = 4\n",
    "batch_size = 6\n",
    "lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, dropout= .05) # input = [Open, HIgh, low, Close] output = [Up, Neutral, Down]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lstm_Classifier(input_size = input_size, hidden_size = hidden_size,\n",
    "                                 num_layers = num_layers,batch_size=batch_size, dropout= .05, num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[[-0.0940, -0.0892,  0.0322, -0.1542],\n         [-0.0925, -0.0890,  0.0293, -0.1356],\n         [-0.0927, -0.0819,  0.0286, -0.1473],\n         [-0.0858, -0.0929,  0.0245, -0.1288],\n         [-0.0964, -0.0844,  0.0318, -0.1473],\n         [-0.0989, -0.0907,  0.0423, -0.1400]],\n\n        [[-0.1274, -0.1215,  0.0629, -0.2094],\n         [-0.1267, -0.1213,  0.0553, -0.1822],\n         [-0.1324, -0.1176,  0.0624, -0.1940],\n         [-0.1284, -0.1213,  0.0552, -0.1715],\n         [-0.1346, -0.1296,  0.0620, -0.2115],\n         [-0.1336, -0.1166,  0.0721, -0.1923]]], grad_fn=<StackBackward>)\ntensor([[[-0.4430,  0.3265,  0.1664],\n         [-0.4156,  0.2622,  0.2493],\n         [-0.4649,  0.3541,  0.1378],\n         [-0.5444,  0.3608,  0.1031],\n         [-0.4267,  0.4208,  0.1373],\n         [-0.3404,  0.4936,  0.1315]],\n\n        [[-0.5383,  0.3504,  0.0992],\n         [-0.4056,  0.3152,  0.2181],\n         [-0.5047,  0.3502,  0.1228],\n         [-0.5228,  0.4002,  0.0870],\n         [-0.4769,  0.3791,  0.1269],\n         [-0.4238,  0.4240,  0.1264]]], grad_fn=<AddBackward0>)\n"
    }
   ],
   "source": [
    "\n",
    "sequence_len = 2\n",
    "inputs = [torch.randn(sequence_len, batch_size, input_size) for _ in range(1)]\n",
    "\n",
    "# initialize the hidden state.\n",
    "hidden = (torch.randn(num_layers, batch_size, hidden_size),\n",
    "          torch.randn(num_layers, batch_size, hidden_size)) \n",
    "\n",
    "for i in inputs:\n",
    "    # Step through the sequence one element at a time.\n",
    "    # after each step, hidden contains the hidden state.\n",
    "    \n",
    "    # out, hidden = lstm(i.view(sequence_len, batch_size, input_size), hidden)\n",
    "    # out, hidden = lstm(i, hidden)\n",
    "    out, hidden = lstm(i)\n",
    "    print(out)\n",
    "    \n",
    "    out = model(i)\n",
    "\n",
    "    print(out)\n",
    "    # print(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.5383,  0.3504,  0.0992],\n        [-0.4056,  0.3152,  0.2181],\n        [-0.5047,  0.3502,  0.1228],\n        [-0.5228,  0.4002,  0.0870],\n        [-0.4769,  0.3791,  0.1269],\n        [-0.4238,  0.4240,  0.1264]], grad_fn=<SelectBackward>)\n"
    }
   ],
   "source": [
    "print(out[-1])"
   ]
  }
 ]
}